# -*- coding: utf-8 -*-
"""IF-IDF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Xh7GRiX0GBXWzgPLITx_2TIgWH_YotW

#Aluna: Gabrielle Louise Ferreira de Melo

##Trabalho: IF-IDF

O trabalho possui código do corpus, bag of word e o IF-IDF
"""

# -*- coding: utf-8 -*-
"""bagofwords.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1brIKnFAdLikXJpa7TDqApfdR8TPm6Ahj
# Aluna: Gabrielle Louise Ferreira de Melo
Para conseguir fazer o Bag Of Words, o corpus precisou ser feito alterações para uma nova versão.
"""

import requests
from bs4 import BeautifulSoup
import spacy

from IPython.display import display
import pandas as pd

#funcao para excluir tudo que não é texto do site
def transformtxt(nlp):
  palavras = 0
  paragrafos = []
  for i, j in zip(nlp.find_all('p'), nlp.find_all('li')):
    palavras += len(str(i.get_text()).split(' ') )
    palavras += len(str(j.get_text()).split(' ') )
    paragrafos.append(str(i.get_text()))
    paragrafos.append(str(j.get_text()))
  return paragrafos, palavras

#pegando as informações dos sites
headers = {'user-agent' : 'Mozila/5.0'}

texto = requests.get("https://www.ibm.com/cloud/learn/natural-language-processing", headers = headers)
textos = texto.text

#urls dos sites
url1 = requests.get("https://www.ibm.com/cloud/learn/natural-language-processing", headers = headers)
url2 = requests.get("https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP", headers = headers)
url3 = requests.get("https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html", headers = headers)
url4 = requests.get("https://en.wikipedia.org/wiki/Natural_language_processing", headers = headers)
url5 = requests.get("https://www.futurelearn.com/info/blog/what-is-natural-language-processing-nlp", headers = headers)

#transformando em texto e html
info_url1 = url1.text
text_html1 = BeautifulSoup(info_url1, 'html.parser')

info_url2 = url2.text
text_html2 = BeautifulSoup(info_url2, 'html.parser')

info_url3 = url3.text
text_html3 = BeautifulSoup(info_url3, 'html.parser')

info_url4 = url4.text
text_html4 = BeautifulSoup(info_url4, 'html.parser')

info_url5 = url5.text
text_html5 = BeautifulSoup(info_url5, 'html.parser')

nlp = spacy.load('en_core_web_sm') # Load the English Model

#executa a função para tirar o que não é necessário
site1_texto = transformtxt(text_html1)[0]
print(transformtxt(text_html1)[1])

site2_texto = transformtxt(text_html2)[0]
print(transformtxt(text_html2)[1])

site3_texto = transformtxt(text_html3)[0]
print(transformtxt(text_html3)[1])

site4_texto = transformtxt(text_html4)[0]
print(transformtxt(text_html4)[1])

site5_texto = transformtxt(text_html5)[0]
print(transformtxt(text_html5)[1])

text_sites = [site1_texto, site2_texto, site3_texto, site4_texto, site5_texto]
len(text_sites)

#aqui ele ira gerar uma lista com as sentenças para o corpus
corpus = []
cont = 0
for texto in text_sites:
  for sentencas in texto:
    corpus.append([])
    for sentenca in nlp(sentencas).sents:
      for palavra in nlp(str(sentenca)):
        palavra = str(palavra).replace(" ", "\n")
        if palavra != "":
          corpus[cont].append(palavra)
      corpus.append([])
      cont += 1

corpus

#fazendo o bag of words
bagOfWords = []

for doc in corpus:
  docArray = []
  for sent in doc:
    words = sent.split(" ")
    docArray.append(words)
    bagOfWords.append(docArray)


frequencia = {}

#verifica a frequencia de cada sentença, vai acrescentando +1 sempre que achar um lexema igual
for doc in bagOfWords:
  for sent in doc:
    for words in sent:
      if words != "":
        if words in frequencia.keys():
          frequencia[words] += 1
        else:
          frequencia[words] = 1

frequencia = sorted(frequencia.items(), key=lambda x:x[1])

#gera a tabela para visualizar o bag of words
df = pd.DataFrame(frequencia)
# displaying the DataFrame
display(df)

"""IF-IDF"""

from collections import defaultdict
from numpy import zeros
import math

"""Para fazer o IF-IDF foi acrescentado uma mudança do bag of words, para criar a matriz com a frequencia de frase, pois na outra versão foi feita própria para mostrar em forma de uma tabela com os lexemas e sua frequencia."""

#alteraçoes do bag of words para o TF IDF
frases = defaultdict(list)

for termo in corpus:
  for termo2 in termo:
    if frases[termo2] == []:
      frases[termo2] = [1]
    else:
      frases[termo2][0] += 1

print(corpus)

#verifica a quantidade de frases na sentença, se tiver frases iguais é adicionado +1 e colocado na matriz matrizTermo
# se não é colocado zero no lugar com a função zeros do numpy
matrizTermo = []
quant_palavra = len(frases)
keys = frases.keys()
cont_geral = 0

for termo in corpus:
  matrizTermo.append(zeros((quant_palavra,), dtype=int))
  for subTermos in termo:
    cont = 0
    for i in frases:
      if i == subTermos:
        matrizTermo[cont_geral][cont] += 1
        break
      cont+=1
  cont_geral +=1

print("Quantidade de palavras: " , quant_palavra)

# fazendo o TF

tf = []
print("TF:")
for i in range(len(corpus)):
  tf.append([])
  for j in range(len(corpus[i])):
    tf[i].append(float(matrizTermo[i][j]) / float(len(corpus[i])))
print(tf)

# fazendo o IDF

idf = []
print("IDF:")
for i in range(len(corpus)):
  idf.append([])
  for j in range(len(corpus[i])):
    idf[i].append( float(math.log(len(corpus))) / float(matrizTermo[i][j] + 1)) #divisão por zero?
print(idf)

#TF-IDF
tf_idf = []
print("TF-IDF:")
for i in range(len(corpus)):
  tf_idf.append([])
  for j in range(len(corpus[i])):
    tf_idf[i].append(tf[i][j] * idf[i][j])
print(tf_idf)